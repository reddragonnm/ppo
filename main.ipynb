{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4748bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "import ale_py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ppo import PPO, DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdaa848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(ale_py)\n",
    "\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=None)\n",
    "env = AtariPreprocessing(env, grayscale_obs=True, scale_obs=True, frame_skip=1, terminal_on_life_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c44d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_obs(obs, crop_top=18):\n",
    "    obs = cv2.resize(obs, (84, 110))\n",
    "    return obs[crop_top: crop_top + 84, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c461b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84) (84, 84)\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(obs, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(preprocess_obs(obs), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(obs.shape, preprocess_obs(obs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6505af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 15\n",
    "total_frames = 1_000_000_000\n",
    "max_ep_frames = 20_000\n",
    "policy_update_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "612b92de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Buffer size: 23 frames\n",
      "Saved model at episode 0 frame 23\n",
      "Episode 10 Frame: 314 Episode Reward: -1.230 Max Reward: -0.510 Entropy Coeff: 0.100\n",
      "Episode 20 Frame: 601 Episode Reward: -1.230 Max Reward: -0.510 Entropy Coeff: 0.100\n",
      "Episode 30 Frame: 951 Episode Reward: -1.230 Max Reward: -0.510 Entropy Coeff: 0.100\n",
      "Episode 40 Frame: 1181 Episode Reward: -1.230 Max Reward: -0.510 Entropy Coeff: 0.100\n",
      "Episode 50 Frame: 1487 Episode Reward: -1.230 Max Reward: -0.510 Entropy Coeff: 0.100\n",
      "Episode 60 Frame: 1791 Episode Reward: -1.230 Max Reward: -0.510 Entropy Coeff: 0.100\n",
      "Episode 70 Frame: 2022 Episode Reward: -1.230 Max Reward: -0.510 Entropy Coeff: 0.100\n",
      "Episode 80 Frame: 2301 Episode Reward: -1.230 Max Reward: -0.510 Entropy Coeff: 0.100\n",
      "Episode 90 Frame: 2587 Episode Reward: -0.510 Max Reward: -0.510 Entropy Coeff: 0.100\n",
      "Episode 100 Frame: 2904 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Training...\n",
      "Buffer size: 2904 frames\n",
      "Saved model at episode 100 frame 2927\n",
      "Episode 110 Frame: 3190 Episode Reward: -0.510 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 120 Frame: 3494 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 130 Frame: 3752 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 140 Frame: 4010 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 150 Frame: 4240 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 160 Frame: 4498 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 170 Frame: 4885 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 180 Frame: 5209 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 190 Frame: 5513 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 200 Frame: 5799 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Training...\n",
      "Buffer size: 2895 frames\n",
      "Saved model at episode 200 frame 5822\n",
      "Episode 210 Frame: 6057 Episode Reward: -0.510 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 220 Frame: 6362 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 230 Frame: 6651 Episode Reward: -0.530 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 240 Frame: 7020 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 250 Frame: 7381 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 260 Frame: 7639 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 270 Frame: 7897 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 280 Frame: 8155 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 290 Frame: 8517 Episode Reward: -1.230 Max Reward: 0.180 Entropy Coeff: 0.100\n",
      "Episode 300 Frame: 8891 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Training...\n",
      "Buffer size: 3120 frames\n",
      "Saved model at episode 300 frame 8942\n",
      "Episode 310 Frame: 9149 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 320 Frame: 9379 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 330 Frame: 9785 Episode Reward: -0.700 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 340 Frame: 10061 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 350 Frame: 10291 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 360 Frame: 10694 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 370 Frame: 11083 Episode Reward: -0.510 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 380 Frame: 11463 Episode Reward: -0.690 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 390 Frame: 11693 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 400 Frame: 12001 Episode Reward: -0.010 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Training...\n",
      "Buffer size: 3082 frames\n",
      "Saved model at episode 400 frame 12024\n",
      "Episode 410 Frame: 12360 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 420 Frame: 12590 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 430 Frame: 13046 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 440 Frame: 13304 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 450 Frame: 13610 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 460 Frame: 13868 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 470 Frame: 14192 Episode Reward: -0.710 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 480 Frame: 14548 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 490 Frame: 14806 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 500 Frame: 15092 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Training...\n",
      "Buffer size: 3091 frames\n",
      "Saved model at episode 500 frame 15115\n",
      "Episode 510 Frame: 15472 Episode Reward: -0.510 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 520 Frame: 15825 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 530 Frame: 16083 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 540 Frame: 16342 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 550 Frame: 16788 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 560 Frame: 17074 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 570 Frame: 17360 Episode Reward: -0.510 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 580 Frame: 17590 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 590 Frame: 17894 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Episode 600 Frame: 18153 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.100\n",
      "Training...\n",
      "Buffer size: 3089 frames\n",
      "Saved model at episode 600 frame 18204\n",
      "Episode 610 Frame: 18592 Episode Reward: -0.710 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 620 Frame: 18946 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 630 Frame: 19252 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 640 Frame: 19556 Episode Reward: -0.510 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 650 Frame: 19947 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 660 Frame: 20283 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 670 Frame: 20541 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 680 Frame: 20989 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 690 Frame: 21303 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 700 Frame: 21589 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Training...\n",
      "Buffer size: 3504 frames\n",
      "Saved model at episode 700 frame 21708\n",
      "Episode 710 Frame: 22064 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 720 Frame: 22488 Episode Reward: -0.690 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 730 Frame: 22774 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 740 Frame: 23248 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 750 Frame: 23552 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 760 Frame: 23932 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 770 Frame: 24395 Episode Reward: 0.490 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 780 Frame: 24625 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 790 Frame: 25036 Episode Reward: -1.230 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Episode 800 Frame: 25370 Episode Reward: -0.710 Max Reward: 1.330 Entropy Coeff: 0.099\n",
      "Training...\n",
      "Buffer size: 3685 frames\n"
     ]
    }
   ],
   "source": [
    "frame = 0\n",
    "num_episodes = 0\n",
    "\n",
    "max_reward = -float('inf')\n",
    "\n",
    "ACTION_MAP = [0, 2, 3]  # NOOP, LEFT, RIGHT\n",
    "\n",
    "while frame < total_frames:\n",
    "    env.reset()\n",
    "    env.step(1)  # FIRE\n",
    "    obs, _, _, _, _ = env.step(1)  # FIRE\n",
    "\n",
    "    frame_stack = deque([preprocess_obs(obs)] * 4, maxlen=4)\n",
    "\n",
    "    ep_reward = 0\n",
    "\n",
    "    for _ in range(max_ep_frames):\n",
    "        action_idx = ppo.get_action(np.array(frame_stack))\n",
    "\n",
    "        action = ACTION_MAP[action_idx]\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        reward -= 0.01  # time penalty\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if done:\n",
    "            reward -= 1.0  # penalty for losing the game\n",
    "\n",
    "        frame_stack.append(preprocess_obs(obs))\n",
    "\n",
    "        ppo.add_reward(reward, done)\n",
    "\n",
    "        ep_reward += reward\n",
    "        frame += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    if num_episodes % policy_update_freq == 0:\n",
    "        print(\"Training...\")\n",
    "        print(f\"Buffer size: {len(ppo.buffer['rewards'])} frames\")\n",
    "\n",
    "        ppo.entropy_coeff = max(0.01, 0.1 - (num_episodes / 10000) * 0.01)      \n",
    "        ppo.update(K=K)\n",
    "\n",
    "        torch.save(ppo.policy.state_dict(), f\"models/model_{num_episodes}.pth\")\n",
    "        print(f\"Saved model at episode {num_episodes} frame {frame}\")\n",
    "\n",
    "    if ep_reward >= max_reward:\n",
    "        torch.save(ppo.policy.state_dict(), f\"models/best_model.pth\")\n",
    "        max_reward = ep_reward\n",
    "        \n",
    "    num_episodes += 1\n",
    "\n",
    "    if num_episodes % 10 == 0:\n",
    "        print(f\"Episode {num_episodes} Frame: {frame} Episode Reward: {ep_reward:.3f} Max Reward: {max_reward:.3f} Entropy Coeff: {ppo.entropy_coeff:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
